{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcuEHtGFoOqt"
   },
   "source": [
    "# Indexing Excercise \n",
    "\n",
    "This exercise has two parts: \n",
    "\n",
    "- In part 1, we are going to index the [MS MARCO](http://www.msmarco.org/) passage collection Pyserini toolkit and explore some features of the index. For this part, you only need to run code and understand it. You will be using the index and code snippets in the next assignment.\n",
    "\n",
    "- In part 2, we are going to write a code for generating an inverted index and index part of MS MARCO collection. For this part, you need to first run the first part (1.1 and 1.2) to build the environment and prepare the data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egQ3UlHWpj0K"
   },
   "source": [
    "## PART 1: Generate the index via Pyserini\n",
    "\n",
    "We use [Anserini](https://github.com/castorini/anserini]) toolkit and its python interface [Pyserini](https://github.com/castorini/pyserini)  to run our experiments. \n",
    "\n",
    "***This part is created based on Anserini/Pyserini tutorials. You can learn more by checking their repositories and tutorials.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Setup the environment\n",
    "\n",
    "Install Pyserini:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyserini\n",
      "  Using cached pyserini-0.22.0-py3-none-any.whl (140.5 MB)\n",
      "Requirement already satisfied: Cython>=0.29.21 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (1.25.2)\n",
      "Requirement already satisfied: pandas>=1.4.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (2.1.0)\n",
      "Requirement already satisfied: pyjnius>=1.4.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (1.5.0)\n",
      "Collecting scikit-learn>=0.22.1\n",
      "  Using cached scikit_learn-1.3.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (1.11.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (4.66.1)\n",
      "Collecting transformers>=4.6.0\n",
      "  Using cached transformers-4.33.1-py3-none-any.whl (7.6 MB)\n",
      "Requirement already satisfied: sentencepiece>=0.1.95 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (0.1.99)\n",
      "Collecting nmslib>=2.1.1\n",
      "  Using cached nmslib-2.1.1.tar.gz (188 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting onnxruntime>=1.8.1\n",
      "  Using cached onnxruntime-1.15.1-cp311-cp311-win_amd64.whl (6.7 MB)\n",
      "Collecting lightgbm>=3.3.2\n",
      "  Using cached lightgbm-4.0.0-py3-none-win_amd64.whl (1.3 MB)\n",
      "Collecting spacy>=3.2.1\n",
      "  Using cached spacy-3.6.1-cp311-cp311-win_amd64.whl (12.0 MB)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pyserini) (6.0.1)\n",
      "Requirement already satisfied: pybind11<2.6.2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from nmslib>=2.1.1->pyserini) (2.6.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from nmslib>=2.1.1->pyserini) (5.9.5)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from onnxruntime>=1.8.1->pyserini) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from onnxruntime>=1.8.1->pyserini) (23.1)\n",
      "Requirement already satisfied: protobuf in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from onnxruntime>=1.8.1->pyserini) (4.24.3)\n",
      "Requirement already satisfied: sympy in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from onnxruntime>=1.8.1->pyserini) (1.12)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pandas>=1.4.0->pyserini) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pandas>=1.4.0->pyserini) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pandas>=1.4.0->pyserini) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from scikit-learn>=0.22.1->pyserini) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from scikit-learn>=0.22.1->pyserini) (3.2.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (3.0.8)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.12-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (0.9.0)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (6.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (2.31.0)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4\n",
      "  Using cached pydantic-2.3.0-py3-none-any.whl (374 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (65.5.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from spacy>=3.2.1->pyserini) (3.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from tqdm->pyserini) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from transformers>=4.6.0->pyserini) (3.12.3)\n",
      "Collecting huggingface-hub<1.0,>=0.15.1\n",
      "  Using cached huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from transformers>=4.6.0->pyserini) (2023.8.8)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from transformers>=4.6.0->pyserini) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from transformers>=4.6.0->pyserini) (0.3.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.6.0->pyserini) (2023.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers>=4.6.0->pyserini) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.1->pyserini) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=3.2.1->pyserini) (2.6.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.4.0->pyserini) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.2.1->pyserini) (2023.7.22)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Using cached blis-0.7.10-cp311-cp311-win_amd64.whl (7.4 MB)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Using cached confection-0.1.2-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy>=3.2.1->pyserini) (8.1.7)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from coloredlogs->onnxruntime>=1.8.1->pyserini) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from jinja2->spacy>=3.2.1->pyserini) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from sympy->onnxruntime>=1.8.1->pyserini) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\daan\\documents\\projecten\\ru-information-retrieval-23-24\\assignments\\env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>=1.8.1->pyserini) (3.4.1)\n",
      "Installing collected packages: nmslib, blis, scikit-learn, pydantic, lightgbm, huggingface-hub, coloredlogs, transformers, pathy, onnxruntime, confection, thinc, spacy, pyserini\n",
      "  Running setup.py install for nmslib: started\n",
      "  Running setup.py install for nmslib: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: nmslib is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Running setup.py install for nmslib did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [11 lines of output]\n",
      "      Dependence list: ['pybind11<2.6.2', 'psutil', \"numpy>=1.10.0,<1.17 ; python_version=='2.7'\", \"numpy>=1.10.0 ; python_version>='3.5'\"]\n",
      "      C:\\Users\\Daan\\Documents\\Projecten\\ru-information-retrieval-23-24\\Assignments\\env\\Lib\\site-packages\\setuptools\\dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
      "        warnings.warn(\n",
      "      running install\n",
      "      C:\\Users\\Daan\\Documents\\Projecten\\ru-information-retrieval-23-24\\Assignments\\env\\Lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "        warnings.warn(\n",
      "      running build\n",
      "      running build_ext\n",
      "      Extra compilation arguments: ['/EHsc', '/openmp', '/O2', '/DVERSION_INFO=\\\\\"2.1.1\\\\\"']\n",
      "      building 'nmslib' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "× Encountered error while trying to install package.\n",
      "╰─> nmslib\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyserini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuqRujlSfnTS"
   },
   "source": [
    "Clone the Anserini repository from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/castorini/anserini.git\n",
    "!cd anserini && git checkout ad5ba1c76196436f8a0e28efdb69960d4873efe3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcU5a-ETrqnT"
   },
   "source": [
    "### 1.2 Get the collection and prepare the files\n",
    "MS MARCO (MicroSoft MAchine Reading COmprehension) is a large-scale dataset that defines many tasks from question answering to ranking. Here we focus on the collection designed for passage re-ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://msmarco.blob.core.windows.net/msmarcoranking/collection.tar.gz -P data/msmarco_passage/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/msmarco_passage/ \n",
    "!tar xvfz data/msmarco_passage/collection.tar.gz -C data/msmarco_passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtK9rHMOwjYx"
   },
   "source": [
    "The original MS MARCO collection is a tab-separated values (TSV) file. We need to convert the collection into the jsonl format that can be processed by Anserini. jsonl files contain JSON object per line.\n",
    "\n",
    "This command generates 9 jsonl files in your data/msmarco_passage/collection_jsonl directory, each with 1M lines (except for the last one, which should have 841,823 lines)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd anserini && python ./src/main/python/msmarco/convert_collection_to_jsonl.py \\\n",
    " --collection_path ../data/msmarco_passage/collection.tsv --output_folder ../data/msmarco_passage/collection_jsonl\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDXFZHf_5lR-"
   },
   "source": [
    "**Check the data!**\n",
    "\n",
    "jsonl files are JSON files with keys id and contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wc -l data/msmarco_passage/collection_jsonl/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 data/msmarco_passage/collection_jsonl/docs00.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAu65qTQ6KNz"
   },
   "source": [
    "Remove the original files to make room for the index. \n",
    "Check the contents of `data/msmarco_passage` before and after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data/msmarco_passage\n",
    "!rm data/msmarco_passage/*.tsv\n",
    "!ls data/msmarco_passage\n",
    "!rm -rf sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uf4Kxgpq6vna"
   },
   "source": [
    "### 1.3 Generate the index using Pyserini\n",
    "\n",
    "\n",
    "Here are some common indexing options with Pyserini (for more options, check Pyserini documentation):\n",
    "\n",
    "```\n",
    "* input: Path to collection\n",
    "* threads: Number of threads to run\n",
    "* collection: Type of Anserini Collection, e.g., LuceneDocumentGenerator, TweetGenerator (subclass of LuceneDocumentGenerator for TREC Microblog)\n",
    "* index: Path to index output\n",
    "* storePositions: Boolean flag to store positions\n",
    "* storeDocvectors: Boolean flag to store document vectors\n",
    "* storeRawDocs: Boolean flag to store raw document text\n",
    "* keepStopwords: Boolean flag to keep stopwords (False by default)\n",
    "* stemmer: Stemmer to use (Porter by default)\n",
    "```\n",
    "\n",
    "We now have everything in place to index the collection. **The indexing speed may vary, the process may take about 10 minutes (or more) in Google Colab.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pyserini.index -collection JsonCollection -generator DefaultLuceneDocumentGenerator -threads 9 \\\n",
    "-input data/msmarco_passage/collection_jsonl -index indexes/lucene-index-msmarco-passage -storePositions -storeDocvectors -storeRaw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWl9RgbZ7dSv"
   },
   "source": [
    "Check the size of the index at the specified destination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0_Nigxvr8DIK",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls indexes\n",
    "!du -h indexes/lucene-index-msmarco-passage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZMAtCEE-f0q"
   },
   "source": [
    "### 1.4 Explore Pyserini index\n",
    "\n",
    "We can now explore the index using the The IndexReader class of Pyserini. \n",
    "\n",
    "Read [Usage of the Index Reader API](https://github.com/castorini/pyserini/blob/master/docs/usage-indexreader.md) notebook for more information on accessing and manipulating an inverted index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pf0oDQfgp7l-"
   },
   "outputs": [],
   "source": [
    "from pyserini.index import IndexReader\n",
    "\n",
    "index_reader = IndexReader('indexes/lucene-index-msmarco-passage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bI2092DyZ94"
   },
   "source": [
    "Compute the collection and document frequencies of a term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term = 'played'\n",
    "\n",
    "# Look up its document frequency (df) and collection frequency (cf).\n",
    "# Note, we use the unanalyzed form:\n",
    "df, cf = index_reader.get_term_counts(term)\n",
    "\n",
    "analyzed_form = index_reader.analyze(term)\n",
    "print(f'Analyzed form of term \"{analyzed_form[0]}\": df={df}, cf={cf}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMwpOyMTulz2"
   },
   "source": [
    "Get basic index statistics of the index.\n",
    "\n",
    "Note that unless the underlying index was built with the `-optimize` option (i.e., merging all index segments into a single segment), unique_terms will show -1 (think what could be reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_reader.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2: Generate the index yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Processing the text\n",
    "\n",
    "We need to process the text, which includes tokenization, stopword removal, and lowercasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = ['a', 'an', 'and', 'are', 'as', 'at', 'be', 'but', 'by', 'for', 'if', 'in', 'into', 'is', 'it', 'no', 'not', 'of', 'on', 'or', 'such', 'that', 'the', 'their', 'then', 'there', 'these', 'they', 'this', 'to', 'was', 'will', 'with']\n",
    "\n",
    "def process(text):\n",
    "    terms = []\n",
    "    # Remove special characters\n",
    "    chars = ['\\'', '.', ':', ',', '!', '?', '(', ')']\n",
    "    for ch in chars:\n",
    "        if ch in text:\n",
    "            text = text.replace(ch, ' ')\n",
    "    \n",
    "    # Lowercasing and stopword removal\n",
    "    for term in text.split():\n",
    "        term = term.lower()\n",
    "        if term not in STOPWORDS:\n",
    "            terms.append(term)\n",
    "    return terms\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Complete the code for Inverted Index\n",
    "\n",
    "Implement the InvertedIndex class. \n",
    "\n",
    "Write the index to a file, where posting list of each term is presented in a line with this format: `Term1 docID1:freq1 docID2:freq2 ...`, e.g., \n",
    "\n",
    "```\n",
    "term1 1:1 4:2 5:1\n",
    "term2 2:1 \n",
    "term3 1:3 3:3 9:2\n",
    "...\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QmSAgk2gQBdb"
   },
   "outputs": [],
   "source": [
    "class InvertedIndex(object):\n",
    "    def __init__(self):\n",
    "        self.index = {}\n",
    "\n",
    "    def add_posting(self, term:str, doc_id:int, count:int):\n",
    "        \"\"\"Adds a posting (term and Document ID) to the index.\"\"\"\n",
    "        # =======Your code=======\n",
    "        \n",
    "        # =======================\n",
    "\n",
    "    def get_posting(self,term:str):\n",
    "        \"\"\"Returns the posting list of the term from the index.\"\"\"\n",
    "        # =======Your code=======\n",
    "\n",
    "        # =======================\n",
    "        \n",
    "    def get_dictionary(self):\n",
    "        \"\"\"Returns the dictionary of the index (unique terms in the index).\"\"\"\n",
    "        # =======Your code=======\n",
    "\n",
    "        # =======================\n",
    "    \n",
    "    def write_to_file(self, filename_index:str):\n",
    "        \"\"\"Writes the index to a textfile.\"\"\"\n",
    "        # =======Your code=======\n",
    "        \n",
    "        # ======================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to test your code. If everything is correct, you should not get errors here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = InvertedIndex()\n",
    "index.add_posting(\"t1\", 1, 2)\n",
    "index.add_posting(\"t1\", 2, 1)\n",
    "index.add_posting(\"t2\", 2, 3)\n",
    "assert len(index.get_dictionary()) == 2\n",
    "assert len(index.get_posting(\"t1\")) == 2\n",
    "assert index.get_posting(\"t3\") == None\n",
    "index.write_to_file(\"data/msmarco_passage/collection_jsonl/text_index.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Index part of the MS MARCO collection\n",
    "\n",
    "Complete the code to process the text and create the index. \n",
    "Note that we are only interested in indexing `docs00.json` file and it takes few minutes to create the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "\n",
    "ind = InvertedIndex()\n",
    "file = \"data/msmarco_passage/collection_jsonl/docs00.json\"\n",
    "index_file = \"data/msmarco_passage/collection_jsonl/tiny_index.txt\"\n",
    "\n",
    "def index(jsonl_file):\n",
    "    with open(jsonl_file, 'r') as f:\n",
    "        for line in f:\n",
    "            doc = json.loads(line)\n",
    "            # =======Your code=======\n",
    "            \n",
    "            \n",
    "            # =======================\n",
    "            \n",
    "index(file)\n",
    "ind.write_to_file(index_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this to test your code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(index_file, 'r') as fp:\n",
    "    assert len(fp.readlines()) == 698784\n",
    "\n",
    "assert len(ind.get_posting(\"pressingly\")) == 3\n",
    "assert len(ind.get_posting(\"veada\")) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handing in\n",
    "\n",
    "Hand in both the result file and the filled-in notebook:\n",
    "\n",
    "- The result file should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_tiny_index.txt\n",
    "- The notebook should be named STUDENTNUMBER_FIRSTNAME_LASTNAME_indexing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "b333cffad54cf9f22a00fb4710e73f1d60aca41dd0187b9ad1417194137be373"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
